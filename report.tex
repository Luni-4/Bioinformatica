% //////////////////// Preambolo /////////////////

% Definizione del documento
\documentclass[12pt,a4paper,oneside,hidelinks]{report}

% Lingue usate nel documento e dizionario per la correzione delle parole
\usepackage[english,italian]{babel}

%Codifica dei font di input e di output
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% Fornisce i comandi per una buona interlinea dei caratteri
\usepackage{setspace}

% Grafica del documento
\usepackage{graphicx}

% Migliore indentazione del testo
\usepackage{indentfirst}

% Gestisce le caption delle immagini
\usepackage{caption}

% Gestiscono la parte matematica del documento
\usepackage{amssymb, amsmath, amsthm}
\DeclareMathOperator{\sgn}{sgn}

% Gestisce il codice del documento
\usepackage{listings}

% Gestisce i link
\usepackage{hyperref}

% Formattazione pagina
\usepackage{geometry}

% Colori
\usepackage{xcolor}

% Pacchetti usati per scrivere lo pseudocodice di un algoritmo
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

%Rimuove la parola "Algorithm #" dallo pseudocodice
%\captionsetup[algorithm]{labelformat=empty}

% Gestione delle multirighe in una tabella
\usepackage{multirow}

% Gestione delle tabelle su più pagine
\usepackage{subcaption}

% //////////////////////////////////////////////////

% Impostare interlinea a 1.5
\renewcommand{\baselinestretch}{1.5}

% Dichiara la funzione argmin non presente di default
\DeclareMathOperator*{\argmin}{argmin} 

% Apertura del pdf con il 100% di zoom
\hypersetup{pdfstartview={XYZ null null 1.00}}

% Impostare i margini della pagina
\geometry{left=2cm,right=2cm, top=2cm, bottom=2cm}

%Impostare font del documento (Latin Modern Roman)
\renewcommand*\rmdefault{lmr}

% //////////////////// DOCUMENTO /////////////////

\begin{document}

% //////////////////// Titolo /////////////////

%Titolo e intestazione
\title{% 
        Predizione della funzione delle proteine \\
        con metodi di Machine Learning}
  
\author{Federico Picetti \\
        Michele Valsesia}

\date{Anno accademico 2017/2018} 

\maketitle

\tableofcontents

% //////////////////// Capitoli /////////////////

\newpage

\section*{Introduzione}
L'obiettivo del progetto consiste nel predire la funzione delle proteine del \textit{Drosophila melanogaster}, un moscerino della frutta nonché organismo modello per gli insetti, usando dei classificatori prodotti da determinati algoritmi di apprendimento.

Per affrontare il problema, si è puntato su modelli semplici, rapidi e che consentano di ottenere una buona valutazione dell'errore di classificazione. Gli algoritmi di apprendimento scelti, che soddisfano le caratteristiche sopradescritte, sono:

\begin{itemize}
    \item Support Vector Machine (SVM)
    \item AdaBoost
\end{itemize}

Lo scopo principale dell'elaborato è quello di descrivere i passaggi e le problematiche affrontate durante lo svolgimento del lavoro. Per fare ciò, si è deciso di associare ad ogni singolo stadio lavorativo un capitolo. Il \autoref{chap:dati} analizza i dati di input, mostrandone la struttura e le diverse modalità di elaborazione.
Nel \autoref{chap:metodi} vengono trattati i metodi di machine learning scelti e la loro relativa implementazione. Il \autoref{chap:risultati}, l'ultimo, presenta i risultati ottenuti dai clasificatori, per mezzo di grafici, e confronta tra loro i due algoritmi per decretare quale sia il migliore.

\chapter{Analisi dei Dati} \label{chap:dati}
Le feature di ingresso sono tratte dalla matrice di adiacenza delle proteine di \emph{Drosophila melanogaster}. La matrice esprime una metrica di similarità fra coppie di proteine.
Gli algoritmi di apprendimento automatico utilizzano l'$ i $-esima riga (o $ i $-esima colonna) della matrice come vettore di feature per l'$ i $-esimo esempio.
Si dispone di 3 distinte matrici di annotazioni, un per ogni ontologia della \emph{GO} (Gene Ontology):
\begin{description}
\item[CC]Cellular Component, 235 classi
\item[BP]Biological Process, 1951 classi
\item[MF]Molecular Function, 234 classi
\end{description}
Si tratta di ontologie multiclasse, per cui ogni proteina può appartenere a una o più classi nella stessa ontologia.
Le matrici di annotazioni $ Y $ riportano le proteine sulle righe e le classi sulle colonne. 

\[ Y_{i,j} =
  \begin{cases}
    1       & \quad \text{se l'elemento } i \text{-esimo appartiene alla classe } j \text{-esima}\\
    0  & \quad \text{altrimenti}\\
  \end{cases}
\]




\chapter{Metodi di Machine Learning} \label{chap:metodi}
Si è deciso di utilizzare tre diversi metodi di Machine Learning: per Support Vector Machine a AdaBoost si sono utilizzate le librerie scikit-learn per Python.

\section{Support Vector Machine}
Si sono provate le SVM della libreria scikit-learn, in particolare l'implementazione SVC\footnote{scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html}.
% Parlare delle impostazioni utilizzate, della funzione di decisione, del bilanciamento, del kernel, di C

\section{AdaBoost}
AdaBoost (adaptive boosting) è un algoritmo incrementale che costruisce una serie di classificatori $ h_{i}:\mathbb{R}^{d}\rightarrow \{-1,+1\} $ appartenenti ad una famiglia $ \mathcal{H} $. 
Il procedimento prevede di addestrare un classificatore di base sul training set, calcolarne l'errore, creare copie del classificatore e addestrarle sullo stesso training set, bilanciando i pesi degli elementi del training set classificati scorrettamente dai classificatori precedenti. I classificatori successivi tenderanno quindi a concentrarsi sui casi più difficili.
Al termine avremo un classificatore nella forma
\[\hat{y}=\sgn(\sum_{i=1}^{T} w_{i}h_{i}(\vec{x}))\]
dove $ \vec{w} $ è un vettore di coefficienti reali (pesi) e $ T $ è il numero di classificatori.

Tipicamente contenere i costi computazionali si sceglie una famiglia $ \mathcal{H} $ di classificatori di base molto semplici. $ T $ può essere un numero fissato o può crescere durante l'apprendimento e fermarsi secondo un dato criterio di stop.

In questo lavoro si è usata l'implementazione \texttt{sklearn.ensemble.AdaBoostClassifier}
\footnote{scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html}.
Questo modulo consente di specificare i seguenti parametri:

\begin{description}
\item[\texttt{base\_estimator}:]la famiglia $ \mathcal{H} $ di \emph{weak learner};
\item[\texttt{n\_estimators}:]la quantità massima di stimatori da costruire, limite oltre il quale il boosting è terminato.
In caso di aderenza perfetta, la procedura di boosting è arrestata prima del raggiungimento del limite.
\item[\texttt{learning\_rate}:] permette di ridurre il contributo di ogni classificatore.
\item[\texttt{algorithm}:]consente di scegliere fra due algoritmi:
\begin{description}
\item[SAMME]Algoritmo di boosting discreto: necessario quando $ \mathcal{H} $ è una famiglia di decisori;
\item[SAMME.R]Algoritmo di boosting reale: tipicamente converge più velocemente, praticabile quando i classificatori $ \mathcal{H} $ restituiscono una probabilità.
\end{description}
\end{description}
 
Si è testato il training set con 5-fold utilizzando alberi di decisione come stimatore di base e fissando il limite di stimatori a 50.
% Di default si usano stump o alberi più profondi?

\section{Implementazione}
Eventualmente parlare di dettagli su come è costruito il codice

\chapter{Analisi dei Risultati} \label{chap:risultati}

\section{Metriche adottate}
Si è deciso di non utilizzare l'accuratezza in quanto non produce buoni risultati se le classi sono sbilanciate.  
Si è adottata la tecnica di cross-validazione 5-fold. Per ogni classe vengono costruiti 5 classificatori simili e addestrati su 4/5 dei dati. Ogni classificatore viene poi testato sul restante 1/5 dei dati.
L'operazione viene eseguita all'interno del modulo \texttt{metrics.py}, 
che a sua volta utilizza la funzione \texttt{cross\_val\_score} contenuta in 
\texttt{sklearn.model\_selection} 
\footnote{scikit-learn.org/stable/modules/generated/sklearn.model\_selection.cross\_val\_score.html}. 

\section{Risultati}

\chapter{Conclusioni}

\end{document}